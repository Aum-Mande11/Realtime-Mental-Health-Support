{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer and tokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Data loading and preprocessing\n",
    "words, classes, documents = [], [], []\n",
    "ignore_words = ['?', '!']\n",
    "data_file = open('intents.json').read()\n",
    "intents = json.loads(data_file)\n",
    "\n",
    "# Tokenizing and lemmatizing\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        w = tokenizer.tokenize(pattern)  # Tokenizing using RegexpTokenizer\n",
    "        words.extend(w)\n",
    "        documents.append((w, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Lemmatize and clean up words\n",
    "words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)\n",
    "\n",
    "# Save words and classes for future use\n",
    "pickle.dump(words, open('texts.pkl', 'wb'))\n",
    "pickle.dump(classes, open('labels.pkl', 'wb'))\n",
    "\n",
    "# Create training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for doc in documents:\n",
    "    bag = [0] * len(words)\n",
    "    pattern_words = [lemmatizer.lemmatize(w.lower()) for w in doc[0]]\n",
    "    \n",
    "    # Create the bag of words\n",
    "    for s in pattern_words:\n",
    "        if s in words:\n",
    "            bag[words.index(s)] = 1\n",
    "    \n",
    "    # Create output row (one-hot encoding for classes)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle and prepare data for training\n",
    "random.shuffle(training)\n",
    "train_x = np.array([x[0] for x in training], dtype=np.float32)\n",
    "train_y = np.array([x[1] for x in training], dtype=np.float32)\n",
    "\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(len(train_x[0]),)))\n",
    "\n",
    "# First hidden layer with more neurons\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))  # Adjusted dropout rate\n",
    "\n",
    "# Second hidden layer with more neurons\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.4))  # Adjusted dropout rate\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile model using Adam optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Early Stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=50, restore_best_weights=True)\n",
    "\n",
    "# Fit the model with validation split\n",
    "hist = model.fit(train_x, train_y, epochs=500, batch_size=5, verbose=1, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Save the model\n",
    "model.save('model.keras')\n",
    "print(\"Model created and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
